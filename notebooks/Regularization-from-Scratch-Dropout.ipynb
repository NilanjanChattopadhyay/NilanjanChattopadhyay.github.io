{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularization-from-Scratch-Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbkx1PPfJeRCbcugKcfJwT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilanjanChattopadhyay/NilanjanChattopadhyay.github.io/blob/master/notebooks/Regularization-from-Scratch-Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2FkKXcYZVJ8",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "The goal of this tutorial is to explore dropout in detail.\n",
        "\n",
        "Prerequisites:\n",
        "\n",
        "*   [Basic understanding of neural networks](https://nilanjanchattopadhyay.github.io/basics/2020/04/01/Deep-Learning-from-Scratch.html)\n",
        "*   Basic PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwd0H1_i8jMk",
        "colab_type": "text"
      },
      "source": [
        "Neural networks containing multiple non-linear hidden layers are universal approximators capable of learning very complicated relationships between inputs and outputs. This makes them extremely flexible and very powerful machine learning systems as we saw on the last [post](https://nilanjanchattopadhyay.github.io/basics/2020/04/01/Deep-Learning-from-Scratch.html). But this flexibility can lead to overfitting, a common problem in neural networks. Large deep learning models often perform far better on training data than on validation data. \n",
        "\n",
        "Let's try to see that in practice. We will start by creating a synthetic data [like we did previously](https://nilanjanchattopadhyay.github.io/basics/2020/04/01/Deep-Learning-from-Scratch.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTz6Be7WQ1pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4CR85RTQ6HB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fgYMrm8Q58e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The parameters of neural networks are initialized at random\n",
        "# By setting a seed we should get the same initial weights every time\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKQA5h1asEWv",
        "colab_type": "text"
      },
      "source": [
        "Here we are creating a *Input* data with 100 dimensions and 10,000 data points randomly generated from $ U(-1, 1] $. Let's denote the dimensions of the *Input* vectors as $ x_{0}, x_{1}, \\dots x_{99} $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSmsNnwpRXCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Input = 2*torch.rand(10000, 100)-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krd4PrUrUHhx",
        "colab_type": "text"
      },
      "source": [
        "## Experiment 1 - High Dimensional Data with Extra Variables and Random Noise\n",
        "\n",
        "We will create a *Target* vector from the input using the following relation:\n",
        "\n",
        "$$ Target = f(x_{0}, x_{1}, \\dots x_{99}) = x_{0}^{0} - x_{1} + x_{2}^{2} - x_{3} \\dots  x_{46}^{46} - x_{47} + x_{48}^{48} - x_{49} + \\epsilon $$\n",
        "$$ \\text{where } \\epsilon \\text{ is random noise} $$\n",
        "\n",
        "There is no reason for choosing this particular relation between *Input* and *Target*. We are trying to create a high-dimensional regression problem with non-linear relationship between the *Input* and *Target* vectors. Though the *Input* has 100 dimensions/variables, note that the relation is dependent on only 50 variables. In practice we will always have extra variables and noise in our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xreNDFOSjh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Target = torch.zeros(10000)\n",
        "for i in range(50):\n",
        "  if i%2 == 0:\n",
        "    Temp = Input[:, i].pow(i)\n",
        "    Target += Temp\n",
        "  else:\n",
        "    Temp = Input[:, i]\n",
        "    Target -= Temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moLGjStfoueX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add Random Noise\n",
        "Noise = 0.5*torch.randn_like(Target)\n",
        "Target += Noise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVW0Cnv2d2l9",
        "colab_type": "code",
        "outputId": "69610da5-4bba-4400-efca-78ff224ccdd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('-'*20, 'Output Cell', '-'*20, '\\n')\n",
        "#Target Dimension\n",
        "print(Target.shape)\n",
        "\n",
        "#Reshape the target variables to appropriate dimension\n",
        "Target = Target.view(10000, 1)\n",
        "\n",
        "#New Target Dimension\n",
        "print(Target.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Output Cell -------------------- \n",
            "\n",
            "torch.Size([10000])\n",
            "torch.Size([10000, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InmAadJAdyVA",
        "colab_type": "text"
      },
      "source": [
        "We will use the first 5,000 elements for training our Neural Network and the remaining 5,000 elements for validation. Let's create the *DataLoader* using our *Input* and *Target*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0L8MTuodxwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "Train_Dataset = TensorDataset(Input[:5000], Target[:5000])\n",
        "Validation_Dataset = TensorDataset(Input[5000:], Target[5000:])\n",
        "\n",
        "batch_size = 1000\n",
        "Train_DataLoader = DataLoader(Train_Dataset, batch_size=batch_size, shuffle=True)\n",
        "Validation_DataLoader = DataLoader(Validation_Dataset, batch_size=batch_size*5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1z6JjffP0T",
        "colab_type": "text"
      },
      "source": [
        "The objective is to create a Neural Network that can identify this non-linear high dimensional relationship.\n",
        "\n",
        "Let’s create a neural network with 2 Hidden Layers and 100 units in each hidden layer and training it for 1000 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeTPE1qUkO5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Neural Network\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, Input_Size=100):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.HiddenLayer_1 = torch.nn.Sequential(torch.nn.Linear(Input_Size, 100), torch.nn.ReLU())\n",
        "    self.HiddenLayer_2 = torch.nn.Sequential(torch.nn.Linear(100, 100), torch.nn.ReLU())\n",
        "    self.OutputLayer = torch.nn.Linear(100, 1)\n",
        "\n",
        "  def forward(self, Input):\n",
        "    Output = self.HiddenLayer_1(Input)\n",
        "    Output = self.HiddenLayer_2(Output)\n",
        "    Output = self.OutputLayer(Output)\n",
        "    return Output\n",
        "\n",
        "# Loss Function - Cross Entropy Loss\n",
        "Loss_Function = torch.nn.MSELoss()\n",
        "\n",
        "def Training(Model, Train_DataLoader, Validation_DataLoader, learning_rate = 0.01, epochs = 1000):\n",
        "  \n",
        "  print('-'*20, 'Output Cell', '-'*20, '\\n')\n",
        "\n",
        "  # Optimizer - Stochastic Gradient Descent\n",
        "  Optimizer = torch.optim.SGD(Model.parameters(), lr=learning_rate)\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    Model.train()\n",
        "    for X, Y in Train_DataLoader:\n",
        "      \n",
        "      # Forward Pass \n",
        "      Predicted = Model(X)\n",
        "\n",
        "      # Compute Loss\n",
        "      Loss = Loss_Function(Predicted, Y)\n",
        "\n",
        "      # Backward Pass\n",
        "      Loss.backward()\n",
        "      Optimizer.step()\n",
        "      Optimizer.zero_grad()\n",
        "\n",
        "    # Calculate Validation Loss\n",
        "    if epoch%100 == 0:\n",
        "      Model.eval()\n",
        "      with torch.no_grad():\n",
        "        Validation_Loss = sum(Loss_Function(Model(X), Y) for X, Y in Validation_DataLoader)\n",
        "        Validation_Loss = Validation_Loss/len(Validation_DataLoader)\n",
        "      print('Epoch: {}. Train Loss: {}. Validation Loss: {}'.format(epoch, Loss, Validation_Loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94B2aOLsmnEr",
        "colab_type": "text"
      },
      "source": [
        "Note that we put the training part of the code in a function. This will make experimentation easier. We just need to run 2 lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUO9bH2efOog",
        "colab_type": "code",
        "outputId": "a3a8313e-7318-4e34-a7d0-7d01ec83eb62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Instantiate Model    \n",
        "Model = NeuralNetwork(Input_Size=100)\n",
        "\n",
        "# Train the Model\n",
        "Training(Model, Train_DataLoader, Validation_DataLoader, learning_rate = 0.01, epochs = 1000)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Output Cell -------------------- \n",
            "\n",
            "Epoch: 0. Train Loss: 14.482518196105957. Validation Loss: 13.808280944824219\n",
            "Epoch: 100. Train Loss: 0.694995105266571. Validation Loss: 0.9867028594017029\n",
            "Epoch: 200. Train Loss: 0.5789515376091003. Validation Loss: 0.9881474375724792\n",
            "Epoch: 300. Train Loss: 0.39037877321243286. Validation Loss: 1.035902500152588\n",
            "Epoch: 400. Train Loss: 0.2847561836242676. Validation Loss: 1.0960569381713867\n",
            "Epoch: 500. Train Loss: 0.21917188167572021. Validation Loss: 1.1488596200942993\n",
            "Epoch: 600. Train Loss: 0.1831873208284378. Validation Loss: 1.2025138139724731\n",
            "Epoch: 700. Train Loss: 0.12253811210393906. Validation Loss: 1.2555932998657227\n",
            "Epoch: 800. Train Loss: 0.09219234436750412. Validation Loss: 1.3032523393630981\n",
            "Epoch: 900. Train Loss: 0.07254666090011597. Validation Loss: 1.34823739528656\n",
            "CPU times: user 54.8 s, sys: 736 ms, total: 55.6 s\n",
            "Wall time: 56 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnQRFDbViMwv",
        "colab_type": "text"
      },
      "source": [
        "Let's breakdown the output into 2 key points -\n",
        "\n",
        "1.   Our network was able to identify the non-linear high dimensional relationship between *Input* and *Output* for training data with *MSE* ~ 0.07.\n",
        "2.   The *MSE* for validation dataset initially decreased to ~ 0.98 and then started to increase as we continued training never decreasing again.\n",
        "\n",
        "This is a clear case of overfitting.\n",
        "\n",
        "Our network overfitted on a data where the validation data is coming from the exact same distribution. One reason for overfitting could be that our model learnt relations that were not present using the variables that were not part of the relationship: $ x_{50}, x_{51}, \\dots x_{99} $. Another reason could be that our model tried to learn the random noise present in our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHj0IANQr-c8",
        "colab_type": "text"
      },
      "source": [
        "## Experiment 2 - High Dimensional Data without Extra Variables or Random Noise\n",
        "\n",
        "What if the we didn't have any noise in the data? And no extra variables? In practice this will be nearly impossible to achieve. But for the sake of experimentation let's do that anyway.\n",
        "\n",
        "Let's recreate the target variable using the following:\n",
        "\n",
        "$$ Target = f(x_{0}, x_{1}, \\dots x_{99}) = x_{0}^{0} - x_{1} + x_{2}^{2} - x_{3} \\dots  x_{96}^{96} - x_{97} + x_{98}^{98} - x_{99} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzQZ9frU2jgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Target = torch.zeros(10000)\n",
        "for i in range(100):\n",
        "  if i%2 == 0:\n",
        "    Temp = Input[:, i].pow(i)\n",
        "    Target += Temp\n",
        "  else:\n",
        "    Temp = Input[:, i]\n",
        "    Target -= Temp\n",
        "\n",
        "#Reshape the target variables to appropriate dimension\n",
        "Target = Target.view(10000, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPmV7yrNtNMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_Dataset = TensorDataset(Input[:5000], Target[:5000])\n",
        "Validation_Dataset = TensorDataset(Input[5000:], Target[5000:])\n",
        "\n",
        "batch_size = 1000\n",
        "Train_DataLoader = DataLoader(Train_Dataset, batch_size=batch_size, shuffle=True)\n",
        "Validation_DataLoader = DataLoader(Validation_Dataset, batch_size=batch_size*5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3scHy310r5YN",
        "colab_type": "code",
        "outputId": "18cee07f-904f-4f82-942f-d5895863197c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Instantiate Model    \n",
        "Model = NeuralNetwork(Input_Size=100)\n",
        "\n",
        "# Train the Model\n",
        "Training(Model, Train_DataLoader, Validation_DataLoader, learning_rate = 0.01, epochs = 1000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Output Cell -------------------- \n",
            "\n",
            "Epoch: 0. Train Loss: 22.0751895904541. Validation Loss: 22.568008422851562\n",
            "Epoch: 100. Train Loss: 0.6760014891624451. Validation Loss: 0.9361695051193237\n",
            "Epoch: 200. Train Loss: 0.45045265555381775. Validation Loss: 0.9307345747947693\n",
            "Epoch: 300. Train Loss: 0.33922550082206726. Validation Loss: 0.9536352753639221\n",
            "Epoch: 400. Train Loss: 0.22653190791606903. Validation Loss: 0.9807875156402588\n",
            "Epoch: 500. Train Loss: 0.17093497514724731. Validation Loss: 1.0150508880615234\n",
            "Epoch: 600. Train Loss: 0.12273935228586197. Validation Loss: 1.044995665550232\n",
            "Epoch: 700. Train Loss: 0.09949737787246704. Validation Loss: 1.0784345865249634\n",
            "Epoch: 800. Train Loss: 0.08115856349468231. Validation Loss: 1.1115176677703857\n",
            "Epoch: 900. Train Loss: 0.06317076832056046. Validation Loss: 1.1423213481903076\n",
            "CPU times: user 55.1 s, sys: 647 ms, total: 55.7 s\n",
            "Wall time: 56 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bJ5YDWLttpA",
        "colab_type": "text"
      },
      "source": [
        "Again let's breakdown the results into 2 key points -\n",
        "\n",
        "*   Our network was able to identify the non-linear high dimensional relationship between Input and Output for training data with MSE ~ 0.06\n",
        "*   The MSE for validation dataset initially decreased to ~ 0.93 and then started to increase as we continued training\n",
        "\n",
        "Even in the absence of any statistical noise our model can overfit. As the dimension of the input increases, the flexibility of our model increases. Higher dimension size means more parameters which makes the model's function selection range is wider making it more prone to overfitting as there will be many different functions that can model the training set almost perfectly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd7OYQ290CSs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "There are various ways to address this problem of overfitting. We can reduce the input dimension or increase training data or use weight penalties of various\n",
        "kinds such as $L1$ and $L2$ regularization. In this post we will be looking at one of the key techniques for reducing overfitting in neural networks - **Dropout**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKgcY6cNeBO2",
        "colab_type": "text"
      },
      "source": [
        "# **Dropout**\n",
        "\n",
        "In 2014 by Srivastava et al. published a paper titled [Dropout: A Simple Way to Prevent Neural Networks from\n",
        "Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) revolutionizing the field of deep learning. In their [paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf), they introduced the idea of randomly dropping units from the neural network during training. They explained that dropout prevents overfitting and provides a way of combining different neural network architectures efficiently. \n",
        "\n",
        "Ensembling multiple models is good way to reduce overfitting and nearly always improves the performance. So, we can train large number of neural networks and average their predictions to get better results. However, this can be very challenging with bigger networks. Training a large is computationally expensive and training multiple models might not be feasible. Ensembling is generally more helpful when the models are not correlated i.e, they should be different from each other. To achieve that the neural networks would need to be of different architectures and trained on different data. This also can be very challenging as we will need to tune every architecture seperately. Also, there may not be enough data available to train different networks on\n",
        "different subsets of the data.\n",
        "\n",
        "Dropout is a technique that provides a way of combining many different neural network architectures efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1N1oGhKo8P9",
        "colab_type": "text"
      },
      "source": [
        "## Dropout during Training\n",
        "\n",
        "Dropout means randomly switching off some hidden units in a neural network while training. During a mini-batch units are randomly removed from the network, along with all its incoming and outgoing connections resulting in a thinned network. Each unit is retained with a fixed probability $p$ independent of other units. This means that the probability of a unit being dropped in a mini-batch will be $1-p$. \n",
        "\n",
        "Since neural networks are a series of aﬃne transformations and non-linearities, an unit can be dropped by multiplying its output value by zero. Thus, dropout can be implemented by multiplying outputs of activations by Bernoulli distributed random variables which take the value 1 with probability $p$ and 0 otherwise. $p$ is a hyperparameter ﬁxed before training.\n",
        "\n",
        "Commonly $p=0.5$ is used for hidden units and $p=0.8$ for input units.\n",
        "\n",
        "Let's look at an example of a neural network with 2 hidden layers *Fig. 1*. \n",
        "\n",
        "<br> <img src=\"https://raw.githubusercontent.com/NilanjanChattopadhyay/NilanjanChattopadhyay.github.io/master/images/MLP.png\" width=\"485\" height=\"400\" title=\"Neural Network\"/>\n",
        "\n",
        "*Fig. 1: Neural Network with 2 input units and 5 hidden units in 2 hidden layers*\n",
        "\n",
        "Let's apply dropout to its hidden layers with $p=0.6$. $p$ is the 'keep probability'. This makes the probability of a hidden unit being dropped equal $1-p=0.4$. Thus with every forward pass 40% of units will be switched off randomly. This will vary with every mini-batch in every epoch: *Fig. 2* and *Fig. 3*. \n",
        "\n",
        "$2^{n}$ thinned neural networks can be generated from a neural network with $n$ units. So training a neural network with dropout can be seen as training a exponentially large number of neural networks from the collection of $2^{n}$ thinned networks where the weights are shared between them.\n",
        "\n",
        "*Fig. 2* and *Fig. 3* illustrates how this network might look like during forward propagation.\n",
        "\n",
        "<br> <img src=\"https://raw.githubusercontent.com/NilanjanChattopadhyay/NilanjanChattopadhyay.github.io/master/images/MLP-Dropout-1.png\" width=\"485\" height=\"400\" title=\"Neural Network\"/>\n",
        "\n",
        "*Fig. 2: Neural Network with 40% hidden units dropped*\n",
        "\n",
        "Different subnetworks will be generated with every forward pass. The dropped units are colored in *red*.\n",
        "\n",
        "<br> <img src=\"https://raw.githubusercontent.com/NilanjanChattopadhyay/NilanjanChattopadhyay.github.io/master/images/MLP-Dropout-2.png\" width=\"485\" height=\"400\" title=\"Neural Network\"/>\n",
        "\n",
        "*Fig. 3: Neural Network with 40% hidden units dropped*\n",
        "\n",
        "Since this network has 10 hidden units, $2^{10}$ different thinned networks are possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9pQQyiKWr6U",
        "colab_type": "text"
      },
      "source": [
        "## Dropout in Practice\n",
        "\n",
        "Let's create the neural network shown in *Fig. 1* and apply dropout to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70dibvO1SFsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mini Batch of 10 elements\n",
        "# Input with 2 dimensions\n",
        "X = torch.rand(10, 2)\n",
        "\n",
        "# Weight for Hidden Layer 1 with 5 units\n",
        "W1 = torch.rand(2, 5)\n",
        "\n",
        "# Bias for Hidden Layer 1 with 5 units\n",
        "B1 = torch.rand(5)\n",
        "\n",
        "# Weight for Hidden Layer 2 with 5 units\n",
        "W2 = torch.rand(5, 5)\n",
        "\n",
        "# Bias for Hidden Layer 2 with 5 units\n",
        "B2 = torch.rand(5)\n",
        "\n",
        "# Weight for Output Layer\n",
        "W3 = torch.rand(5, 1)\n",
        "\n",
        "# Bias for Output Layer\n",
        "B3 = torch.rand(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYtSiwxEabx8",
        "colab_type": "text"
      },
      "source": [
        "Let's assume the activation function is $ReLU$ for our network. If we don't apply dropout, we will have a normal forward pass for this mini-batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD1NL2IxTR7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mutiply Input by W1 and add bias B1\n",
        "H1 = X@W1 + B1\n",
        "# Apply ReLU activation\n",
        "H1.clamp_(0)\n",
        "\n",
        "# H1 is the output from Hidden Layer 1\n",
        "# Mutiply H1 by W2 and add bias B2\n",
        "H2 = H1@W2 + B2\n",
        "# Apply ReLU activation\n",
        "H2.clamp_(0)\n",
        "\n",
        "# H2 is the output from Hidden Layer 2\n",
        "# Mutiply H2 by W3 and add bias B3\n",
        "Out = H2@W3 + B3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFMvg4a9cjkx",
        "colab_type": "text"
      },
      "source": [
        "To apply dropout we will create a binary vector, commonly called as binary mask, where $1$'s will represent the units to keep and $0$'s will represnt the units to drop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyEXSGCLUAGe",
        "colab_type": "code",
        "outputId": "4d63f339-50fd-407c-ac60-835296cf3eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print('-'*20, 'Output Cell', '-'*20, '\\n')\n",
        "\n",
        "# Mask for keeping units with probability p\n",
        "# Mask for dropping units with probability 1-p\n",
        "mask = torch.zeros(1, 5).bernoulli_(0.6)\n",
        "mask"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Output Cell -------------------- \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgWubKwCloxL",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the output of Hidden Layer 1 without dropout. The output vector of this layer will have 5 elements; 1 from every hidden unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouch9tIndHDg",
        "colab_type": "code",
        "outputId": "1e918277-1a52-41df-ed18-6a96852a36d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "print('-'*20, 'Output Cell', '-'*20, '\\n')\n",
        "\n",
        "# Output of Hidden Layer 1\n",
        "H1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Output Cell -------------------- \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4571, 0.9301, 1.5057, 1.0187, 1.0740],\n",
              "        [2.0775, 1.2849, 1.8956, 1.4305, 1.3876],\n",
              "        [1.3839, 0.8882, 1.4594, 0.9700, 1.0363],\n",
              "        [1.1605, 0.7488, 1.2952, 0.8139, 0.8683],\n",
              "        [1.2070, 0.7238, 1.2188, 0.8101, 0.6482],\n",
              "        [1.2837, 0.7614, 1.2542, 0.8569, 0.6575],\n",
              "        [1.9915, 1.2499, 1.8704, 1.3829, 1.4107],\n",
              "        [1.0826, 0.7468, 1.3334, 0.7908, 1.0303],\n",
              "        [0.8492, 0.5682, 1.0942, 0.6055, 0.6988],\n",
              "        [1.1232, 0.7963, 1.4127, 0.8354, 1.1750]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z79fW5UFZH1u",
        "colab_type": "text"
      },
      "source": [
        "To apply the dropout mask, we do an element wise product. We can look at the result and confirm that unit 2, unit 4 and unit 5 have been turned off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN3tDv0BdXTY",
        "colab_type": "code",
        "outputId": "18d901d3-4ead-49ed-b09f-fd96ed0978f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "print('-'*20, 'Output Cell', '-'*20, '\\n')\n",
        "\n",
        "mask*H1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Output Cell -------------------- \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4571, 0.0000, 1.5057, 0.0000, 1.0740],\n",
              "        [2.0775, 0.0000, 1.8956, 0.0000, 1.3876],\n",
              "        [1.3839, 0.0000, 1.4594, 0.0000, 1.0363],\n",
              "        [1.1605, 0.0000, 1.2952, 0.0000, 0.8683],\n",
              "        [1.2070, 0.0000, 1.2188, 0.0000, 0.6482],\n",
              "        [1.2837, 0.0000, 1.2542, 0.0000, 0.6575],\n",
              "        [1.9915, 0.0000, 1.8704, 0.0000, 1.4107],\n",
              "        [1.0826, 0.0000, 1.3334, 0.0000, 1.0303],\n",
              "        [0.8492, 0.0000, 1.0942, 0.0000, 0.6988],\n",
              "        [1.1232, 0.0000, 1.4127, 0.0000, 1.1750]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI-X-azIZHyc",
        "colab_type": "text"
      },
      "source": [
        "Putting all the code together to implement dropout from scratch!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPntznxZdgNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mutiply Input by W1 and add bias B1\n",
        "H1 = X@W1 + B1\n",
        "# Apply ReLU activation\n",
        "H1.clamp_(0)\n",
        "# Apply dropout\n",
        "mask = torch.zeros(1, 5).bernoulli_(0.6)\n",
        "H1 = mask * H1\n",
        "\n",
        "# H1 is the output from Hidden Layer 1\n",
        "# Mutiply H1 by W2 and add bias B2\n",
        "H2 = H1@W2 + B2\n",
        "# Apply ReLU activation\n",
        "H2.clamp_(0)\n",
        "# Apply dropout\n",
        "mask = torch.zeros(1, 5).bernoulli_(0.6)\n",
        "H2 = mask * H2\n",
        "\n",
        "# H2 is the output from Hidden Layer 2\n",
        "# Mutiply H2 by W3 and add bias B3\n",
        "Out = H2@W3 + B3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckGJKUApZHvs",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "\n",
        "One of the most common way to combine multiple models is to take arithmatic mean of the predictions from each model. But in dropout there can be exponentially many thinned models and it becomes unfeasible to store and average predictions from all the model. \n",
        "\n",
        "The predictions of the combined models can be approximated by averaging together the output from few thinned networks. 10–20 such subnetworks are often suﬃcient to obtain good performance. \n",
        "\n",
        "Another good approximation can be achieved by taking geometric mean of the predictions. Arithmetic mean and geometric mean often performs comparably when ensembling. More details can be found in this [paper](https://arxiv.org/pdf/1312.6197.pdf).\n",
        "\n",
        "Note that since the geometric mean of multiple predictions might not be a probability distribution. Therefore, a condition is placed that none of the submodels can assign a probability of 0 to any event. Also, the resulting distribution is normalized.\n",
        "\n",
        "> **Arithmetic Mean**\n",
        "$$ p_{ensemble} = \\sum_{\\mu}^{}p(\\mu)p(y|x, \\mu) $$\n",
        "\n",
        "> **Geometric Mean**\n",
        "$$ p_{ensemble} = \\bigg(\\prod_{\\mu}^{}p(\\mu)p(y|x, \\mu)\\bigg)^{\\frac{1}{2^{d}}} $$\n",
        "\n",
        "<br> $where$\n",
        "<br> $\\mu$ represents the mask vector\n",
        "<br> $p(\\mu)$ is the probability distribution used to sample $\\mu$ during training\n",
        "<br> $p(y|x, \\mu)$ prediction of thinned network\n",
        "<br> $d$ is the number of units that may be dropped\n",
        "\n",
        "Inference for dropout is achived by approximating the geometric mean. If $p$ is the probability of a unit being retained during training, then the outgoing weights of that unit are multiplied by $p$ at test time. The idea is to keep the expected output value from the unit same during training and test time. This approximates the geometric mean of predictions  of the entire ensemble. Empirically training a network with dropout and using approximate averaging method at test time improves generalization and reduces overfitting and can work as well as Monte-Carlo Model Averaging.\n",
        "\n",
        "Let's implement the inference part of dropout along with the training part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESOBNyAev_Rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############# TRAINING #############\n",
        "# Mutiply Input by W1 and add bias B1\n",
        "H1 = X@W1 + B1\n",
        "# Apply ReLU activation\n",
        "H1.clamp_(0)\n",
        "# Apply dropout\n",
        "mask = torch.zeros(1, 5).bernoulli_(0.6)\n",
        "H1 = mask * H1\n",
        "\n",
        "# H1 is the output from Hidden Layer 1\n",
        "# Mutiply H1 by W2 and add bias B2\n",
        "H2 = H1@W2 + B2\n",
        "# Apply ReLU activation\n",
        "H2.clamp_(0)\n",
        "# Apply dropout\n",
        "mask = torch.zeros(1, 5).bernoulli_(0.6)\n",
        "H2 = mask * H2\n",
        "\n",
        "# H2 is the output from Hidden Layer 2\n",
        "# Mutiply H2 by W3 and add bias B3\n",
        "Out = H2@W3 + B3\n",
        "\n",
        "\n",
        "############# INFERENCE #############\n",
        "# Mutiply Input by W1 and add bias B1\n",
        "H1 = X@W1 + B1\n",
        "# Apply ReLU activation\n",
        "H1.clamp_(0)\n",
        "# Scaling the output of Hidden Layer 1\n",
        "H1 = H1*0.6\n",
        "\n",
        "# H1 is the output from Hidden Layer 1\n",
        "# Mutiply H1 by W2 and add bias B2\n",
        "H2 = H1@W2 + B2\n",
        "# Apply ReLU activation\n",
        "H2.clamp_(0)\n",
        "# Scaling the output of Hidden Layer 2\n",
        "H2 = H2*0.6\n",
        "\n",
        "# H2 is the output from Hidden Layer 2\n",
        "# Mutiply H2 by W3 and add bias B3\n",
        "Out = H2@W3 + B3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_wjpSYpZHtA",
        "colab_type": "text"
      },
      "source": [
        "## Inverted Dropout\n",
        "\n",
        "Till now we have applied dropout as per the [dropout paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). However, most of the libaries, like PyTorch, implements **'Inverted Dropout'**. \n",
        "\n",
        "In invereted dropout the scaling is applied during training. Inverted dropout randomly retains some activations with probability $p$ similar to traditional dropout. Then scaling is done by multiplying the output of retained units $1/p$.\n",
        "Since, scaling is done during training, no changes are required during evaluation.\n",
        "\n",
        "Let's apply scaling during training to implement inverted dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-hoI1NO1Toe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############# TRAINING #############\n",
        "# Mutiply Input by W1 and add bias B1\n",
        "H1 = X@W1 + B1\n",
        "# Apply ReLU activation\n",
        "H1.clamp_(0)\n",
        "# Apply dropout\n",
        "mask = torch.zeros(1, 5).bernoulli_(0.6)\n",
        "H1 = mask * H1\n",
        "# Scaling the output of Hidden Layer 1\n",
        "H1 = H1/0.6\n",
        "\n",
        "# H1 is the output from Hidden Layer 1\n",
        "# Mutiply H1 by W2 and add bias B2\n",
        "H2 = H1@W2 + B2\n",
        "# Apply ReLU activation\n",
        "H2.clamp_(0)\n",
        "# Apply dropout\n",
        "mask = torch.zeros(1, 5).bernoulli_(0.6)\n",
        "H2 = mask * H2\n",
        "# Scaling the output of Hidden Layer 2\n",
        "H2 = H2/0.6\n",
        "\n",
        "# H2 is the output from Hidden Layer 2\n",
        "# Mutiply H2 by W3 and add bias B3\n",
        "Out = H2@W3 + B3\n",
        "\n",
        "\n",
        "############# INFERENCE #############\n",
        "# Mutiply Input by W1 and add bias B1\n",
        "H1 = X@W1 + B1\n",
        "# Apply ReLU activation\n",
        "# No Scaling Required\n",
        "H1.clamp_(0)\n",
        "\n",
        "# H1 is the output from Hidden Layer 1\n",
        "# Mutiply H1 by W2 and add bias B2\n",
        "H2 = H1@W2 + B2\n",
        "# Apply ReLU activation\n",
        "# No Scaling Required\n",
        "H2.clamp_(0)\n",
        "\n",
        "# H2 is the output from Hidden Layer 2\n",
        "# Mutiply H2 by W3 and add bias B3\n",
        "Out = H2@W3 + B3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmvlba0Y1Gmu",
        "colab_type": "text"
      },
      "source": [
        "## Experiment 1 - Revisited With Dropout\n",
        "\n",
        "Let's try to implement dropout for **Experiment 1** to reduce the overfitting. Since we didn't change *Input* and *Noise* tensors, we can recreate the exact same target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtFQoKJQ79l7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Target = torch.zeros(10000)\n",
        "for i in range(50):\n",
        "  if i%2 == 0:\n",
        "    Temp = Input[:, i].pow(i)\n",
        "    Target += Temp\n",
        "  else:\n",
        "    Temp = Input[:, i]\n",
        "    Target -= Temp\n",
        "\n",
        "# Add Random Noise\n",
        "Target += Noise\n",
        "\n",
        "#Reshape the target variables to appropriate dimension\n",
        "Target = Target.view(10000, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6WNJyXp79if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_Dataset = TensorDataset(Input[:5000], Target[:5000])\n",
        "Validation_Dataset = TensorDataset(Input[5000:], Target[5000:])\n",
        "\n",
        "batch_size = 1000\n",
        "Train_DataLoader = DataLoader(Train_Dataset, batch_size=batch_size, shuffle=True)\n",
        "Validation_DataLoader = DataLoader(Validation_Dataset, batch_size=batch_size*5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP-7KRMZ8nKT",
        "colab_type": "text"
      },
      "source": [
        "We will implement dropout in our *NeuralNetwork* class. \n",
        "\n",
        "There are 2 key points we need to note in the code below:\n",
        "\n",
        "*   **self.training**: While in training mode, PyTorch sets self.training to *True* and in evaluation mode it is set as false. Since the behaviour of dropout is different for training and inference, model.train() and model.eval() should be used to get the correct results.\n",
        "*   Till now we have considered $p$ as the keep probability and $1-p$ as the probability of dropout. In our code we will consider $p$ as the probability of dropout and $1-p$ as the probability of survival and change the scaling formula accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JFdIA8h79fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Neural Network\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, Input_Size=100, Dropout=0.5):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.HiddenLayer_1 = torch.nn.Sequential(torch.nn.Linear(Input_Size, 100), torch.nn.ReLU())\n",
        "    self.HiddenLayer_2 = torch.nn.Sequential(torch.nn.Linear(100, 100), torch.nn.ReLU())\n",
        "    self.OutputLayer = torch.nn.Linear(100, 1)\n",
        "    self.Dropout = Dropout\n",
        "\n",
        "  def forward(self, Input):\n",
        "    Output = self.HiddenLayer_1(Input)\n",
        "    \n",
        "    # Use dropout only when training the model\n",
        "    if self.training:\n",
        "      DropoutMask = torch.zeros(1, 100).bernoulli_(1-self.Dropout)\n",
        "      Output = (Output*DropoutMask)/(1-self.Dropout) #Scaling with 1-p\n",
        "    \n",
        "    Output = self.HiddenLayer_2(Output)\n",
        "    \n",
        "    # Use dropout only when training the model\n",
        "    if self.training:\n",
        "      DropoutMask = torch.zeros(1, 100).bernoulli_(1-self.Dropout)\n",
        "      Output = (Output*DropoutMask)/(1-self.Dropout) #Scaling with 1-p\n",
        "\n",
        "    Output = self.OutputLayer(Output)\n",
        "    return Output\n",
        "\n",
        "# Loss Function - Cross Entropy Loss\n",
        "Loss_Function = torch.nn.MSELoss()\n",
        "\n",
        "def Training(Model, Train_DataLoader, Validation_DataLoader, learning_rate = 0.01, epochs = 1000):\n",
        "  \n",
        "  print('-'*20, 'Output Cell', '-'*20, '\\n')\n",
        "\n",
        "  # Optimizer - Stochastic Gradient Descent\n",
        "  Optimizer = torch.optim.SGD(Model.parameters(), lr=learning_rate)\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    Model.train()\n",
        "    for X, Y in Train_DataLoader:\n",
        "      \n",
        "      # Forward Pass \n",
        "      Predicted = Model(X)\n",
        "\n",
        "      # Compute Loss\n",
        "      Loss = Loss_Function(Predicted, Y)\n",
        "\n",
        "      # Backward Pass\n",
        "      Loss.backward()\n",
        "      Optimizer.step()\n",
        "      Optimizer.zero_grad()\n",
        "\n",
        "    # Calculate Validation Loss\n",
        "    if epoch%100 == 0:\n",
        "      Model.eval()\n",
        "      with torch.no_grad():\n",
        "        Validation_Loss = sum(Loss_Function(Model(X), Y) for X, Y in Validation_DataLoader)\n",
        "        Validation_Loss = Validation_Loss/len(Validation_DataLoader)\n",
        "      print('Epoch: {}. Train Loss: {}. Validation Loss: {}'.format(epoch, Loss, Validation_Loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbMswFwe79dq",
        "colab_type": "code",
        "outputId": "0bd8d183-d03e-45db-b86b-afe3eb6711e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Instantiate Model    \n",
        "Model = NeuralNetwork(Input_Size=100, Dropout=0.5)\n",
        "\n",
        "# Train the Model\n",
        "Training(Model, Train_DataLoader, Validation_DataLoader, learning_rate = 0.01, epochs = 1000)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Output Cell -------------------- \n",
            "\n",
            "Epoch: 0. Train Loss: 13.633894920349121. Validation Loss: 12.919963836669922\n",
            "Epoch: 100. Train Loss: 2.7887604236602783. Validation Loss: 1.3544188737869263\n",
            "Epoch: 200. Train Loss: 1.0066168308258057. Validation Loss: 1.0069533586502075\n",
            "Epoch: 300. Train Loss: 1.1311012506484985. Validation Loss: 0.9397183656692505\n",
            "Epoch: 400. Train Loss: 0.9387773871421814. Validation Loss: 0.9175894260406494\n",
            "Epoch: 500. Train Loss: 0.7152908444404602. Validation Loss: 0.9644955992698669\n",
            "Epoch: 600. Train Loss: 0.6427285671234131. Validation Loss: 0.9647810459136963\n",
            "Epoch: 700. Train Loss: 1.0787990093231201. Validation Loss: 0.9440218806266785\n",
            "Epoch: 800. Train Loss: 0.7154536843299866. Validation Loss: 0.9039661884307861\n",
            "Epoch: 900. Train Loss: 0.8444961905479431. Validation Loss: 0.9420482516288757\n",
            "CPU times: user 59.4 s, sys: 1.18 s, total: 1min\n",
            "Wall time: 1min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2lj4yg-1Gj9",
        "colab_type": "text"
      },
      "source": [
        "Compare the current results with the previous results for the same experiment. By applying dropout, we improved validation loss from 0.98 to 0.90. The gap between MSE for train and validation is also reduced.\n",
        "\n",
        "Dropout significantly reduced overfitting for **Experiment 1** and improved generalization resulting in better performance on validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFhkUH7f1Gh0",
        "colab_type": "text"
      },
      "source": [
        "## Experiment 2 - Revisited With Dropout\n",
        "\n",
        "Let's implement dropout for **Experiment 2**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWJAp-jR_x3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Target = torch.zeros(10000)\n",
        "for i in range(100):\n",
        "  if i%2 == 0:\n",
        "    Temp = Input[:, i].pow(i)\n",
        "    Target += Temp\n",
        "  else:\n",
        "    Temp = Input[:, i]\n",
        "    Target -= Temp\n",
        "\n",
        "#Reshape the target variables to appropriate dimension\n",
        "Target = Target.view(10000, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc36kMd-_xz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_Dataset = TensorDataset(Input[:5000], Target[:5000])\n",
        "Validation_Dataset = TensorDataset(Input[5000:], Target[5000:])\n",
        "\n",
        "batch_size = 1000\n",
        "Train_DataLoader = DataLoader(Train_Dataset, batch_size=batch_size, shuffle=True)\n",
        "Validation_DataLoader = DataLoader(Validation_Dataset, batch_size=batch_size*5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ryROWtQAsTx",
        "colab_type": "text"
      },
      "source": [
        "PyTorch's torch.nn.Module provides a dropout class that can be used directly. It automatically handles mask creation and scaling. The probability argument in **torch.nn.Dropout** is the probability of units being dropped out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M469WNqd_xxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Neural Network\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, Input_Size=100, Dropout=0.5):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.HiddenLayer_1 = torch.nn.Sequential(torch.nn.Linear(Input_Size, 100), torch.nn.ReLU(), torch.nn.Dropout(Dropout))\n",
        "    self.HiddenLayer_2 = torch.nn.Sequential(torch.nn.Linear(100, 100), torch.nn.ReLU(), torch.nn.Dropout(Dropout))\n",
        "    self.OutputLayer = torch.nn.Linear(100, 1)\n",
        "\n",
        "  def forward(self, Input):\n",
        "    Output = self.HiddenLayer_1(Input)    \n",
        "    Output = self.HiddenLayer_2(Output)\n",
        "    Output = self.OutputLayer(Output)\n",
        "    return Output\n",
        "\n",
        "# Loss Function - Cross Entropy Loss\n",
        "Loss_Function = torch.nn.MSELoss()\n",
        "\n",
        "def Training(Model, Train_DataLoader, Validation_DataLoader, learning_rate = 0.01, epochs = 1000):\n",
        "  \n",
        "  print('-'*20, 'Output Cell', '-'*20, '\\n')\n",
        "\n",
        "  # Optimizer - Stochastic Gradient Descent\n",
        "  Optimizer = torch.optim.SGD(Model.parameters(), lr=learning_rate)\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    Model.train()\n",
        "    for X, Y in Train_DataLoader:\n",
        "      \n",
        "      # Forward Pass \n",
        "      Predicted = Model(X)\n",
        "\n",
        "      # Compute Loss\n",
        "      Loss = Loss_Function(Predicted, Y)\n",
        "\n",
        "      # Backward Pass\n",
        "      Loss.backward()\n",
        "      Optimizer.step()\n",
        "      Optimizer.zero_grad()\n",
        "\n",
        "    # Calculate Validation Loss\n",
        "    if epoch%100 == 0:\n",
        "      Model.eval()\n",
        "      with torch.no_grad():\n",
        "        Validation_Loss = sum(Loss_Function(Model(X), Y) for X, Y in Validation_DataLoader)\n",
        "        Validation_Loss = Validation_Loss/len(Validation_DataLoader)\n",
        "      print('Epoch: {}. Train Loss: {}. Validation Loss: {}'.format(epoch, Loss, Validation_Loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlTRgMwfA6Tl",
        "colab_type": "code",
        "outputId": "7ee39447-fb73-4b24-e23e-7826eb881ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Instantiate Model    \n",
        "Model = NeuralNetwork(Input_Size=100, Dropout=0.4)\n",
        "\n",
        "# Train the Model\n",
        "Training(Model, Train_DataLoader, Validation_DataLoader, learning_rate = 0.01, epochs = 1000)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Output Cell -------------------- \n",
            "\n",
            "Epoch: 0. Train Loss: 25.26565170288086. Validation Loss: 22.792816162109375\n",
            "Epoch: 100. Train Loss: 1.9885936975479126. Validation Loss: 0.9988459944725037\n",
            "Epoch: 200. Train Loss: 1.5658793449401855. Validation Loss: 0.9235767722129822\n",
            "Epoch: 300. Train Loss: 1.2947447299957275. Validation Loss: 0.8972725868225098\n",
            "Epoch: 400. Train Loss: 1.313409447669983. Validation Loss: 0.8803383708000183\n",
            "Epoch: 500. Train Loss: 1.2465885877609253. Validation Loss: 0.878248929977417\n",
            "Epoch: 600. Train Loss: 1.2394073009490967. Validation Loss: 0.8703423738479614\n",
            "Epoch: 700. Train Loss: 1.0760048627853394. Validation Loss: 0.8612719774246216\n",
            "Epoch: 800. Train Loss: 0.9913138747215271. Validation Loss: 0.8619877099990845\n",
            "Epoch: 900. Train Loss: 0.9837628602981567. Validation Loss: 0.8579367995262146\n",
            "CPU times: user 59.9 s, sys: 1.33 s, total: 1min 1s\n",
            "Wall time: 1min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTxhe18V1Geo",
        "colab_type": "text"
      },
      "source": [
        "Again we see significant reduction in overfitting and improved performance on validation set - MSE dropped from 0.93 to 0.85. Dropout can also be combined with other forms of regularization to get further improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjhpu3Ry1GPZ",
        "colab_type": "text"
      },
      "source": [
        "## Salient Features\n",
        "\n",
        "Training a neural network with dropout can be seen as training multiple networks where the weights are shared between them. Since the architecture of the model changes with every mini-batch, every unit learns to perform well regardless of which other hidden units are present in the model. This makes the units robust independently that is good in many settings and therby preventing co-adaptation. **This results in better performance and improved generalization error compared to the performance obtained by ensembles of independent models**. You can get more details from this [paper](https://arxiv.org/pdf/1312.6197.pdf).\n",
        "\n",
        "Dropout can also be seen as a way of adding noise to the states of hidden units.  As dropout causes destruction of the some information from input, units are forced to learn other features and make use of all the knowledge about the input.\n",
        "\n",
        "Dropout can also be modified by multiplying the activations with random variables drawn from other distributions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n35GYy0P_mbw",
        "colab_type": "text"
      },
      "source": [
        "## Caution\n",
        "\n",
        "We need to exercise some caution when implementing dropout in neural networks. \n",
        "\n",
        "\n",
        "*   A neural network with dropout usually needs to be trained longer as the parameter updates are very noisy.\n",
        "\n",
        "*   A larger neural network is required when applying dropout. Dropout is a regularization technique and reduces the expressiveness of neural networks. The combination of larger network with dropout typically results in lower validation error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYyCiA769NoZ",
        "colab_type": "text"
      },
      "source": [
        "# **Summary**\n",
        "\n",
        "Dropout is computationally inexpensive but powerful technique for improving neural networks by reducing overfitting. Dropout trains an ensemble of multiple thinned subnetworks that can be formed by removing hidden or input units from a base network. This prevents networks from building brittle co-adaptations do not generalize well.\n",
        "\n",
        "In this tutorial we explored dropout in detail and implemented it from scratch. We can now leverage this remarkably effective technique to improve the performance of neural nets in a wide variety of application."
      ]
    }
  ]
}